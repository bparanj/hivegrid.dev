Create a cron playbook, cron_job.yml inside ansible/playbooks/maintain:

```yml
- name: Setup a cron job to print the timestamp every midnight
  hosts: all
  become: yes # Use sudo to manage cron jobs
  tasks:
    - name: Ensure cron is installed (Ubuntu)
      ansible.builtin.apt:
        name: cron
        state: present

    - name: Schedule cron job to print the timestamp at midnight
      ansible.builtin.cron:
        name: "Print timestamp"
        job: "date \"+\\%F \\%T\" >> /var/log/timestamp.log"
        minute: "0"
        hour: "0"
        day: "*"
        month: "*"
        weekday: "*"
        user: root
        state: present
```

From the ansible folder, run the playbook:

```sh
ansible-playbook  playbooks/maintain/cron_job.yml -i inventory.ini
```

View the crontab for the root user:

```
ubuntu@ip-10-0-10-4:~$ sudo crontab -l -u root
#Ansible: Print timestamp
0 0 * * * date "+\%F \%T" >> /var/log/timestamp.log
```

Verify that /var/log/timestamp.log has timestamp to make sure CRON is setup correctly.

```
cat /var/log/timestamp.log
```

Output:

```
2024-04-29 00:00:01
```

Database backup:

```yml
- hosts: all
  become: yes
  become_user: root
  become_method: sudo
  become_flags: '-i'  # This helps in some cases where the environment needs to be cleaner

  vars:
    backup_dir: /var/backups/postgres

  tasks:
    - name: Ensure cron is installed (Ubuntu)
      ansible.builtin.apt:
        name: cron
        state: present

    - name: Create backup directory
      file:
        path: "{{ backup_dir }}"
        state: directory
        owner: postgres
        group: postgres
        mode: '0755'

    - name: Create a CRON job for PostgreSQL backup
      cron:
        name: "Backup PostgreSQL database every midnight"
        minute: "0"
        hour: "0"
        job: >
          /usr/bin/pg_dump -U {{ db_user }} -Fc {{ db_name }} > {{ backup_dir }}/{{ db_name }}_`date +\%Y-\%m-\%d_\%H-\%M-\%S`.dump
        state: present
        user: postgres

    - name: Set PostgreSQL user password
      postgresql_user:
        name: "{{ db_user }}"
        password: "{{ db_password }}"
      become_user: postgres
```

```
ansible-playbook playbooks/maintain/backup-db.yml -e "db_user=your_db_user db_password=your_db_password db_name=your_db_name"
```

ansible-playbook playbooks/maintain/backup-db.yml -e "db_user=deploy db_password=password db_name=capt_production"

ansible-galaxy install ome.postgresql_backup


---
- hosts: all
  become: yes

  vars:
    postgresql_backup_dir: "/var/backups/postgres"
    postgresql_backup_database: "{{ db_name }}"
    postgresql_backup_username: "{{ db_user }}"
    postgresql_backup_password: "{{ db_password }}"

  roles:
    - ome.postgresql_backup


ansible-playbook -i inventory.ini playbooks/maintain/backup-db.yml -e "postgresql_backup_username=deploy postgresql_backup_password=password db_name=capt_production"

Check if cron is setup correctly:

sudo crontab -l -u postgres


sudo ls -l /var/backups/postgres


Yes, the cron job syntax you've posted is generally correct for what you're trying to accomplish, which is to take a backup of the `capt_production` database using `pg_dump` every day at midnight. However, there's a small modification needed to correctly handle the output redirection in a cron environment.

### Corrected Command:

Here's the command with the necessary correction:

```bash
0 0 * * * /usr/bin/pg_dump -U deploy -Fc capt_production > /var/backups/postgres/capt_production_$(date +\%Y-\%m-\%d_\%H-\%M-\%S).dump
```

### Key Changes:

1. **Use `$(...)` for Command Substitution**: It's better to use `$(...)` for command substitution instead of backticks `` `...` ``. It’s more modern and allows for nesting, which can be useful in more complex scenarios.

2. **Escaping Percent Signs**: You correctly escaped the percent signs (`%`) as `\%`, which is necessary in cron jobs because unescaped percent signs are treated as newline characters.

### Explanation of Each Part:

- `0 0 * * *`: This specifies when the cron job runs. `0 0 * * *` means it runs at 0 minutes and 0 hours of every day of every month, effectively midnight each day.
- `/usr/bin/pg_dump`: This is the path to the `pg_dump` command, which is used for taking backups of PostgreSQL databases.
- `-U deploy`: This option specifies the username `deploy` to connect to the PostgreSQL database.
- `-Fc`: This option tells `pg_dump` to use the custom format for the backup file, which is more flexible and robust than plain SQL dumps.
- `> /var/backups/postgres/capt_production_$(date +\%Y-\%m-\%d_\%H-\%M-\%S).dump`: This redirects the output of the `pg_dump` command to a file. The filename includes a timestamp to ensure it's unique and provides an easy way to identify when the backup was taken.

Make sure the `postgres` user or the user under which the cron job runs has the appropriate permissions to write to the `/var/backups/postgres/` directory. Also, ensure that the `deploy` user has access to perform the `pg_dump` on the `capt_production` database. If any authentication errors arise, you may need to check the PostgreSQL user permissions or the `pg_hba.conf` configuration.

Ansible galaxy library for database backups: https://github.com/ome/ansible-role-postgresql-backup

## Troubleshooting Issues

To help diagnose and ensure that everything is set up correctly for your PostgreSQL backup using Ansible, you can create a detailed script. This script will be divided into two parts: the first will check system configurations and variables directly on your system, and the second will be an Ansible playbook to verify the role's functionality and debug configurations.

### Part 1: System Configuration Checks (Shell Script)

You can run this script on your target machine to ensure the necessary conditions for your Ansible role are met. Save this as `check_system.sh`.

```bash
#!/bin/bash

# Check if PostgreSQL is running
echo "Checking if PostgreSQL is running..."
sudo systemctl status postgresql | grep "active (running)" > /dev/null && echo "PostgreSQL is running." || echo "PostgreSQL is NOT running."

# Check available disk space
echo "Checking disk space for backup directory..."
df -h /var/backups/postgres

# Check directory permissions
echo "Checking permissions for /var/backups/postgres..."
ls -ld /var/backups/postgres

# Check if crontab for postgres is set up
echo "Checking cron jobs for postgres user..."
sudo crontab -l -u postgres

# Echo environment variables that might affect PostgreSQL
echo "Environment variables for the current user related to PostgreSQL:"
env | grep PG

# Check connectivity to PostgreSQL
echo "Checking PostgreSQL connectivity..."
sudo -u postgres psql -c '\conninfo'

# Output ends here
echo "System checks complete."
```

### Part 2: Ansible Playbook for Debugging (Ansible YAML)

This playbook checks variables and tests the PostgreSQL backup functionality provided by the `ome.postgresql_backup` role. Save this as `debug_playbook.yml`.

```yaml
---
- hosts: all
  become: yes

  vars_prompt:
    - name: "db_name"
      prompt: "Enter the database name"
      private: no
    - name: "db_user"
      prompt: "Enter the database username"
      private: no
    - name: "db_password"
      prompt: "Enter the database password"
      private: yes

  tasks:
    - name: Print backup directory
      debug:
        msg: "Backup directory is set to {{ postgresql_backup_dir }}."

    - name: Print database name
      debug:
        msg: "Database name is {{ postgresql_backup_database }}."

    - name: Print database username
      debug:
        msg: "Database username is {{ postgresql_backup_username }}."

    - name: Attempt to create a file in the backup directory
      file:
        path: "{{ postgresql_backup_dir }}/test_file"
        state: touch
      ignore_errors: yes

    - name: Check file creation result
      debug:
        msg: "Test file creation in the backup directory succeeded."
      when: not ansible_failed_result

    - name: Include the ome.postgresql_backup role to perform a test backup
      include_role:
        name: ome.postgresql_backup

  vars:
    postgresql_backup_dir: "/var/backups/postgres"
    postgresql_backup_database: "{{ db_name }}"
    postgresql_backup_username: "{{ db_user }}"
    postgresql_backup_password: "{{ db_password }}"
```

### Instructions for Use:

1. **Run the Shell Script:**
   - Save the shell script to the target machine.
   - Make it executable with `chmod +x check_system.sh`.
   - Execute it by running `./check_system.sh`.

2. **Run the Ansible Playbook:**
   - Save the Ansible playbook to your control machine.
   - Run the playbook with `ansible-playbook debug_playbook.yml`.

These scripts will provide comprehensive checks and output to diagnose issues with your PostgreSQL backup configuration using the `ome.postgresql_backup` role. Make sure you review the output carefully to understand any potential misconfigurations or errors.

If you're seeing that one of your database backup files is 0 bytes while the other is not, it typically indicates a problem with the backup process that generated the empty file. Here are some common reasons and troubleshooting steps you can take:

### 1. **Check Backup Command Execution**

The command used in the cron job might have encountered an error. The `pg_dump` command might fail for various reasons including incorrect user permissions, database connection issues, or incorrect command syntax. Here’s how you can manually run the command to check for errors:

```bash
sudo -u postgres /usr/bin/pg_dump -U postgres -Fc capt_production > /var/backups/postgres/test_dump.dump
```

Replace `postgres` with the actual username and `capt_production` with your database name as needed. This will help you see if any errors are printed out during the manual execution.

### 2. **Inspect Cron Job Configuration**

There might be an issue in how the cron job is set up. For instance, environmental variables that are available in your interactive shell (like `$PATH`) might not be available in the cron environment. This can cause the `pg_dump` command to fail when run via cron.

- Ensure the full path to `pg_dump` is used (which you seem to have done).
- You can also modify the cron job to log errors to a file to get more insight:

```bash
0 0 * * * /usr/bin/pg_dump -U deploy -Fc capt_production > /var/backups/postgres/capt_production_$(date +\%Y-\%m-\%d_\%H-\%M-\%S).dump 2> /var/backups/postgres/backup_error.log
```

This modification will redirect standard error to a log file, which can help diagnose the problem.

### 3. **Database Connection Issues**

The database user may not have the correct permissions or the database itself might not be accessible due to network issues or misconfigurations in `pg_hba.conf`.

- Check the database connection settings and user permissions.
- Look into `pg_hba.conf` for any misconfigured rules that might prevent connections from the host where the cron job runs.

### 4. **Filesystem Permissions or Space Issues**

If the filesystem where `/var/backups/postgres` resides is full or if there are permissions issues, writing the dump file could fail.

- Check disk space using `df -h`.
- Ensure that the `postgres` user has write permissions in the `/var/backups/postgres` directory.

### 5. **Check System and Application Logs**

Look at system logs (`/var/log/syslog` or `/var/log/messages`) and PostgreSQL logs to find any related entries about failures.

### Manual Testing and System Review

Running the backup command manually as shown above and checking the logs should provide insights into what might be going wrong. Adjusting the cron job to capture error output as suggested will further aid in troubleshooting.

The Ansible playbook snippet you provided looks generally correct for setting up a task using the `ome.postgresql_backup` role from Ansible Galaxy. This role is designed to automate PostgreSQL backups, utilizing the provided variables. However, there are a few areas to review or check if the backup files are not being created as expected or if you're encountering issues like the 0-byte file problem:

### 1. **Check Role Variables**

First, make sure that the variables `db_name`, `db_user`, and `db_password` are being passed correctly to the playbook. These variables need to be defined elsewhere in your Ansible setup (e.g., in the playbook file, in an included vars file, or passed as extra variables at runtime). Ensure they are correctly assigned and relevant to your PostgreSQL setup.

```yaml
vars:
  postgresql_backup_dir: "/var/backups/postgres"
  postgresql_backup_database: "{{ db_name }}"
  postgresql_backup_username: "{{ db_user }}"
  postgresql_backup_password: "{{ db_password }}"
```

### 2. **Role Compatibility and Configuration**

Ensure that the `ome.postgresql_backup` role is compatible with your Ansible and OS versions. Also, check if there are additional required configurations or dependencies:

- **Dependencies**: Make sure all dependencies for the role are satisfied.
- **Documentation**: Review the documentation of the `ome.postgresql_backup` role for any required settings or known issues that might not be obvious from the role's basic configuration.

### 3. **Debugging Information**

Add debugging tasks to your playbook to print out the values being used for critical variables, ensuring they are what you expect:

```yaml
tasks:
  - debug:
      msg:
        - "Backup directory: {{ postgresql_backup_dir }}"
        - "Database name: {{ postgresql_backup_database }}"
        - "Database user: {{ postgresql_backup_username }}"
```

### 4. **Check Execution and Permissions**

Verify that:
- The playbook is executed with sufficient privileges (`become: yes` ensures tasks are run with root privileges, which is typically necessary for installing packages and writing to certain directories).
- The `postgres` user (or whichever user runs the PostgreSQL service) has the necessary permissions to access the backup directory and write to it.

### 5. **Manual Role Execution**

Try running the role manually with a minimal playbook to isolate any issues from other playbook complexities. For example:

```yaml
- hosts: all
  become: yes
  vars:
    postgresql_backup_dir: "/var/backups/postgres"
    postgresql_backup_database: "example_db"
    postgresql_backup_username: "postgres"
    postgresql_backup_password: "example_password"
  roles:
    - ome.postgresql_backup
```

Replace `"example_db"`, `"postgres"`, and `"example_password"` with actual values to test.

### 6. **Examine Logs and Outputs**

After running the playbook:
- Check Ansible's output for any errors or warnings.
- Check the PostgreSQL logs for any backup-related errors.
- Verify that the role’s tasks are completing as expected and not silently failing or skipping important steps.

### 7. **Review Role's Task Definitions**

Look at the source code of the `ome.postgresql_backup` role (typically found in your roles directory under `ome.postgresql_backup/tasks/main.yml` after installation) to understand exactly what it does and how it handles errors. This might give you clues about any configuration issues or bugs.

By following these steps, you can better diagnose and resolve issues with the `ome.postgresql_backup` role in your Ansible playbook.

From the output you provided, there are a few key points to address:

### PostgreSQL Service Status
The output for `PostgreSQL is NOT running` seems inconsistent with the later output from `systemctl status postgresql`, which shows that PostgreSQL is in an `active (exited)` state. This typically means the PostgreSQL service's main process isn't running, but the service has not failed and is not considered inactive. It's a common state for services that start up, spawn other processes, and then the main process exits.

**Resolution:** You might want to ensure that PostgreSQL is indeed running correctly. You can attempt to restart the service and check its status directly:

```bash
sudo systemctl restart postgresql
sudo systemctl status postgresql
```

These commands restart PostgreSQL and then get the status to confirm it's actively running processes.

### Checking Permissions
The backup directory permissions are `drwx------`, indicating that only the `postgres` user has access to this directory. If other users or processes need access, you'll need to adjust these permissions.

**Resolution:** If it's safe and other users need to read or manage files in this directory, consider adjusting permissions like so:

```bash
sudo chmod 755 /var/backups/postgres
```
This command gives read and execute permissions to all users and write permission to the owner.

### Cron Job Configuration
The cron job is configured to run as expected, confirming the setup seems correct. However, if the `pg_dump` command results in a 0-byte file, it might not be executing correctly due to environmental differences in the cron environment.

**Resolution:** Directly run the cron command in the terminal to check for errors and ensure it executes as expected:

```bash
sudo -u postgres /usr/bin/pg_dump -U deploy -Fc capt_production > /var/backups/postgres/test_dump.dump
```

Also, consider adding error logging to the cron job:

```bash
0 0 * * * /usr/bin/pg_dump -U deploy -Fc capt_production > /var/backups/postgres/capt_production_$(date +\%Y-\%m-\%d_\%H-\%M-\%S).dump 2> /var/backups/postgres/backup_error.log
```

This command will write errors to `backup_error.log`, which can help identify issues.

### Summary
- **Ensure PostgreSQL is actively running** with processes managed by systemd.
- **Adjust permissions** if necessary to fit your security and operational needs.
- **Test the cron job command manually** to ensure `pg_dump` runs as expected.
- **Add error logging to the cron job** to capture any failures during execution.

These steps should help clarify and potentially resolve the issue with the backup process.

The status "active (exited)" in the context of a systemd service, like the PostgreSQL service, can be a bit confusing but it's quite common and has a specific meaning.

### Understanding "active (exited)"
- **Active**: This indicates that the service is considered active by systemd.
- **Exited**: This means the main process associated with the service has completed its execution and exited.

For many services, especially those that perform a specific action and then terminate (like script-based services), this is a normal and expected state. However, for a service like PostgreSQL, which typically runs a persistent daemon (server process), "active (exited)" might suggest that the main service was started successfully but didn't continue running as expected. This could be due to the service's nature (it might spawn child processes and then the parent process exits), or it could indicate an issue where the service doesn't stay up as expected.

### Possible Scenarios for PostgreSQL
1. **Service Initialization Script**: For PostgreSQL, the "active (exited)" status might occur if the initialization script completes successfully, and the control is handed over to another process, but systemd only tracks the initial script. The actual PostgreSQL daemon might still be running fine, managed by another process.

2. **Configuration or Startup Issue**: If there’s a misconfiguration or a problem during the startup, the main PostgreSQL process might start and then immediately exit, which could also lead to this status. However, this would usually be accompanied by errors or log entries indicating what went wrong.

### Checking the Actual Status of PostgreSQL
To truly understand whether PostgreSQL is running correctly, you should check beyond the systemd status. Here are a few ways to confirm that PostgreSQL is operational:

- **Check Process List**:
  ```bash
  ps aux | grep postgres
  ```
  This command checks if the PostgreSQL processes are actively running.

- **Connect to PostgreSQL**:
  ```bash
  sudo -u postgres psql -c '\conninfo'
  ```
  This command tries to connect to the PostgreSQL database server and provide connection information, confirming if the server is operational.

- **Review Logs**:
  PostgreSQL logs are typically located in `/var/log/postgresql/` or directly accessible through the PostgreSQL command line. Reviewing these logs can provide insights into what happens when the service starts.
  ```bash
  sudo cat /var/log/postgresql/postgresql-*.log
  ```

If PostgreSQL is running correctly and other checks (like connecting to the database) work without issues, the "active (exited)" status might simply reflect the way systemd interprets the service's start-up script behavior. If there are problems, however, the logs and direct process checks should reveal more detailed information about what is going wrong.

The output from `ps aux | grep postgres` shows that PostgreSQL is indeed running and has multiple processes active, which are typical for a running PostgreSQL server:

- **Main Postgres Process**: This is the main PostgreSQL server process (`/usr/lib/postgresql/16/bin/postgres`) which initializes and manages other necessary subprocesses for database operations.
- **Checkpointer**: Manages the writing of in-memory data to disk to ensure data integrity.
- **Background Writer**: Helps reduce disk I/O by writing out "dirty" (modified) buffers from the buffer cache to the disk in the background.
- **WAL Writer**: Responsible for writing out WAL (Write-Ahead Logging) records to disk to ensure transactions are recorded persistently.
- **Autovacuum Launcher**: Manages the autovacuum workers which help to recover disk space and optimize the database by automatically vacuuming the database tables.
- **Logical Replication Launcher**: Manages the logical replication workers if logical replication is configured.

### Implications:
The presence of these processes confirms that PostgreSQL is actively running and performing its typical database management functions. The `active (exited)` status reported by `systemctl` is, therefore, likely due to the systemd service management behavior as discussed previously:

1. **Systemd Behavior**: When the PostgreSQL service is started, it likely spawns these processes and then the initial script exits, which is why `systemctl` reports `active (exited)`. This behavior is normal for services that are designed to spawn a process and then exit, leaving the process running independently.

### Conclusion:
From the information provided, PostgreSQL is running normally and handling typical database tasks. The `systemctl` status of `active (exited)` is not indicative of an error but rather a characteristic of how PostgreSQL is managed as a service under systemd. You can continue to operate your PostgreSQL server as usual, knowing that it is running and functioning as expected.

### Suggested Actions:
- **Monitoring and Logs**: Continue to monitor PostgreSQL logs periodically to ensure there are no hidden issues.
- **Regular Maintenance**: Ensure that maintenance tasks like vacuuming and backups are occurring as scheduled to keep PostgreSQL running smoothly.
- **Backup Validity**: Given the previous issue with backup file sizes, it would be prudent to verify the backups regularly to ensure data integrity and successful backup operations.

Great! If you're providing the database name, user, and password via command line arguments when running your Ansible playbook, you should ensure that these variables are correctly referenced in your playbook and passed effectively to the roles or tasks that require them.

Here's how you can adjust your playbook to accept these variables from the command line using Ansible's `extra-vars` (`-e`) option during execution:

### Playbook Setup

Your playbook can be set up like this, assuming these variables are not defined anywhere else in the playbook:

```yaml
---
- hosts: all
  become: yes

  vars:
    postgresql_backup_dir: "/var/backups/postgres"

  roles:
    - ome.postgresql_backup
```

In this configuration, the variables `postgresql_backup_database`, `postgresql_backup_username`, and `postgresql_backup_password` are expected to be provided at runtime.

### Running the Playbook with Variables

When running the playbook, you would pass the necessary variables like so:

```bash
ansible-playbook your_playbook.yml -e "db_name=your_database_name db_user=your_database_user db_password=your_database_password"
```

Make sure to replace `your_database_name`, `your_database_user`, and `your_database_password` with the actual values.

### Ensuring Variables are Passed to Roles

If you are using these variables specifically within an Ansible role (like `ome.postgresql_backup`), ensure that the role is set up to accept these variables, or map them within your playbook. For example:

```yaml
---
- hosts: all
  become: yes

  vars:
    postgresql_backup_dir: "/var/backups/postgres"
    postgresql_backup_database: "{{ db_name }}"
    postgresql_backup_username: "{{ db_user }}"
    postgresql_backup_password: "{{ db_password }}"

  roles:
    - ome.postgresql_backup
```

This setup ensures that the role receives the necessary variables as expected.

### Debugging Tips

1. **Use Debug Statements**: To ensure that your variables are being passed correctly, you can add debug tasks before the role execution:

   ```yaml
   tasks:
     - name: Debug database variables
       debug:
         msg: 
           - "Database Name: {{ db_name }}"
           - "Database User: {{ db_user }}"
           - "Database Password: {{ db_password }}"
   ```

2. **Verbose Mode**: Run your playbook in verbose mode (`ansible-playbook -vvv your_playbook.yml`) to see detailed output, which can help in tracing how variables are being passed and handled.

Following these steps should help ensure that your variables are correctly passed and used within your Ansible playbook and roles.
